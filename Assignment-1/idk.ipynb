{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# AI2000\n",
    "## Assignment-1 \n",
    "### AI20BTECH11006\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 1\n",
    "## 1. k-NN: (8 marks) In k-nearest neighbors (k-NN), the classification is achieved by majority vote in the vicinity of data. Given n points, imagine two classes of data each of n/2 points, which are overlapped to some extent in a 2-dimensional space.\n",
    "### (a) (1 mark) Describe what happens to the training error (using all available data) when the neighbor size k varies from n to 1.\n",
    "Solution: \n",
    "When $k$ is $1$, the training error will be $0$ because we are finding error on training data, the closest point would be the same data point itself. For $k$ between $2$ to $n-1$ we can't predict the error, there will be some error because of overlap. For $k=n$ there will be a tie and which will cause an error of $0.5$ average.\n",
    "\n",
    "### (b) (2 marks) Predict and explain with a sketch how the generalization error (e.g. holding out some data for testing) would change when k varies? Explain your reasoning.\n",
    "Solution:\n",
    "For $k=1$ the generalisation error would be high because of over-fitting, As $k$ grows the error would start decreasing and reach a minimal value after which it will start increasing again because we would start getting more and more data points of other classes. The image for the sketch is given [here](https://drive.google.com/file/d/1ChEX8aRgkZGGsnLwfy8Kai_6ZNjWpI1t/view?usp=sharing)\n",
    "\n",
    "### (c) (2 marks) Give two reasons (with sound justification) why k-NN may be undesirable when the input dimension is high.\n",
    "Solution:\n",
    "1) The difference in distance between two pairs of data points becomes small\n",
    "2) The test data size must increase a lot to accomodate higher input dimension\n",
    "\n",
    "### (d) (3 marks) Is it possible to build a univariate decision tree (with decisions at each node of the form “is x > a”, “is x < b”, “is y > c”, or “is y < d” for any real constants a, b, c, d) which classifies exactly similar to a 1-NN using the Euclidean distance measure? If so, explain how. If not, explain why not.\n",
    "Solution: \n",
    "No.\n",
    "The univariate decision tree can only form perpendicular lines to distinguish between classes, while in case of 1-NN we have voronoi diagram which may not have only perpendicular lines. A pictorial representation for same can be found [here](https://drive.google.com/file/d/1Cwugt_LgEbKn7MAQw9XY8eHZzMCdSAB8/view?usp=sharing)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "import csv\n",
    "\n",
    "# Enter You Name Here\n",
    "myname = \"Chirag-Mehta\" \n",
    "\n",
    "def entropy(data):\n",
    "    last=len(data[0])-1\n",
    "    nZero=0\n",
    "    nOne=0\n",
    "    for i in range(len(data)):\n",
    "        # print(float(data[i][last]))\n",
    "        if float(data[i][last]) == 0:\n",
    "            nZero=nZero+1\n",
    "        else :\n",
    "            nOne=nOne+1\n",
    "    # print(nZero,nOne)\n",
    "    p1 = nOne/(nOne+nZero)\n",
    "    p2=1-p1\n",
    "    if p1!=0:\n",
    "        p1=-p1*np.log(p1)\n",
    "    if p2!=0:\n",
    "        p2=-p2*np.log(p2)\n",
    "    return p1+p2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "# Implement your decision tree below\n",
    "import numpy as np\n",
    "class DecisionTree():\n",
    "    tree = {}\n",
    "\n",
    "    def learn(self, training_set):\n",
    "        # implement this function\n",
    "        \n",
    "        self.tree = {\"feature\":None,\"threshold\":None,\"leftNode\":None,\"rightNode\":None} \n",
    "        BestInfoGain= -np.inf\n",
    "        leftNode=DecisionTree\n",
    "        rightNode=DecisionTree\n",
    "        Feature=None\n",
    "        Threshold=None\n",
    "        for feature in range(len(training_set[0])-1):\n",
    "            thresholdValues=[]\n",
    "            valuesInAttribute=[float(x) for x in (training_set[idx][feature] for idx in range(len(training_set)))]\n",
    "            minimumThreshold=min(valuesInAttribute)\n",
    "            maximumThreshold=max(valuesInAttribute)\n",
    "            x=np.linspace( minimumThreshold, maximumThreshold, 12, endpoint=False)[1:]\n",
    "            \n",
    "            for threshold in x:\n",
    "                right=[]\n",
    "                left=[]                \n",
    "                for index in range(len(valuesInAttribute)):\n",
    "                    if valuesInAttribute[index] < threshold:\n",
    "                        left.append( [training_set[index][x] for x in [*range(0,feature),*range(feature+1)]])\n",
    "                    else:\n",
    "                        right.append( [training_set[index][x] for x in [*range(0,feature),*range(feature+1)]])\n",
    "                infoGain=entropy(training_set)-entropy(left)-entropy(right)\n",
    "                print(\"Entropy left and right\" ,entropy(left),entropy(right))\n",
    "                if infoGain > BestInfoGain:\n",
    "                    print(\"well yes\")\n",
    "                    Feature=feature\n",
    "                    Threshold=threshold\n",
    "                    print(\"Feature and Threshold\", Feature,Threshold)\n",
    "\n",
    "                    leftNode=left\n",
    "                    rightNode=right\n",
    "                    BestInfoGain=infoGain\n",
    "        print(Feature,Threshold)\n",
    "        self.tree['feature']=Feature\n",
    "        self.tree['threshold']=Threshold\n",
    "        self.tree['leftNode']=DecisionTree()\n",
    "        self.tree['rightNode']=DecisionTree()\n",
    "        self.tree['leftNode'].learn(leftNode)\n",
    "        self.tree['leftNode'].learn(leftNode)\n",
    "\n",
    "                \n",
    "    # implement this function\n",
    "    def classify(self, test_instance):\n",
    "        result = 0 # baseline: always classifies as 0\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    \n",
    "def accuracy(predicted, actual):\n",
    "    crt=0\n",
    "    for i in range(len(predicted)):\n",
    "        if predicted[i] == actual[i]:\n",
    "            crt+=1\n",
    "    return crt/len(predicted)*100\n",
    "\n",
    "def test_split(index, value, dataset):\n",
    "\tleft, right = list(), list()\n",
    "\tfor row in dataset:\n",
    "\t\tif row[index] < value:\n",
    "\t\t\tleft.append(row)\n",
    "\t\telse:\n",
    "\t\t\tright.append(row)\n",
    "\treturn left, right\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "# Load data set\n",
    "with open(\"wine-dataset.csv\") as f:\n",
    "    next(f, None)\n",
    "    data = [tuple(line) for line in csv.reader(f, delimiter=\",\")]\n",
    "# X=list(map(lambda x: x[:10], data))\n",
    "# Y=list(map(lambda x: x[11], data))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "import random\n",
    "\n",
    "def run_decision_tree():\n",
    "\n",
    "    # Load data set\n",
    "    with open(\"wine-dataset.csv\") as f:\n",
    "        next(f, None)\n",
    "        data = [tuple(line) for line in csv.reader(f, delimiter=\",\")]\n",
    "    random.shuffle(data)\n",
    "#     print(\"Number of records:\",data)\n",
    "\n",
    "    # Split training/test sets\n",
    "    # You need to modify the following code for cross validation.\n",
    "    K = 10\n",
    "    training_set = [x for i, x in enumerate(data) if i % K != 9]\n",
    "    test_set = [x for i, x in enumerate(data) if i % K == 9]\n",
    "    print(len(training_set[0]))\n",
    "    tree = DecisionTree()\n",
    "    # Construct a tree using training set\n",
    "    tree.learn( training_set )\n",
    "\n",
    "    # Classify the test set using the tree we just constructed\n",
    "    results = []\n",
    "    for instance in test_set:\n",
    "        result = tree.classify( instance[:-1] )\n",
    "        results.append( result == instance[-1])\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = float(results.count(True))/float(len(results))\n",
    "    print(\"accuracy: %.4f\" % accuracy)\n",
    "    \n",
    "\n",
    "    # Writing results to a file (DO NOT CHANGE)\n",
    "    f = open(myname+\"result.txt\", \"w\")\n",
    "    f.write(\"accuracy: %.4f\" % accuracy)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "12\n",
      "Entropy left and right 0.0 0.0\n",
      "well yes\n",
      "Feature and Threshold 0 4.666666666666666\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.4080956627963265 0.0\n",
      "Entropy left and right 0.07397580682208409 0.0\n",
      "Entropy left and right 0.03162684535160897 0.0\n",
      "Entropy left and right 0.027572168474835883 0.0\n",
      "Entropy left and right 0.02690973955331232 0.0\n",
      "Entropy left and right 0.02658861213224057 0.0\n",
      "Entropy left and right 0.026573282312620322 0.0\n",
      "Entropy left and right 0.026542679572236827 0.0\n",
      "Entropy left and right 0.02653758647567376 0.0\n",
      "Entropy left and right 0.02653758647567376 0.0\n",
      "Entropy left and right 0.02653758647567376 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "Entropy left and right 0.0 0.0\n",
      "0 4.666666666666666\n",
      "None None\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'type' object is not subscriptable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-1a7c39b8420a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mrun_decision_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-101-1a7c39b8420a>\u001b[0m in \u001b[0;36mrun_decision_tree\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Construct a tree using training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtraining_set\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Classify the test set using the tree we just constructed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-99-91847fe5cd39>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, training_set)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'leftNode'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecisionTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rightNode'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecisionTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'leftNode'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleftNode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'leftNode'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleftNode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-99-91847fe5cd39>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, training_set)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'leftNode'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecisionTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rightNode'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecisionTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'leftNode'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleftNode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'leftNode'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleftNode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-99-91847fe5cd39>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, training_set)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mFeature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mThreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mthresholdValues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mvaluesInAttribute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'type' object is not subscriptable"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}