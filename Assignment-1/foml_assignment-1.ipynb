{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "with open(\"wine-dataset.csv\") as f:\n",
    "    next(f, None)\n",
    "    Tdata=[]\n",
    "    for line in csv.reader(f,delimiter=\",\"):\n",
    "        row=[float(x) for x in line]\n",
    "        Tdata.append(row)\n",
    "\n",
    "# entropy function\n",
    "def entropy(data):\n",
    "    \n",
    "    lastIndex=len(data[0])-1\n",
    "    n0,n1=0,0\n",
    "    for i in data:\n",
    "        if i[lastIndex]==0:\n",
    "            n0=n0+1\n",
    "        else:\n",
    "            n1=n1+1\n",
    "    p1=n0/(n1+n0)\n",
    "    p2=1-p1\n",
    "    if p1 != 0:\n",
    "        p1=-p1*np.log(p1)\n",
    "    if p2 != 0:\n",
    "        p2=-p2*np.log(p2)\n",
    "    return p1+p2\n",
    "\n",
    "\n",
    "# The Best split function\n",
    "def BestSplit(Data):\n",
    "    RowSize=len(Data[0])\n",
    "    if RowSize<=1:\n",
    "        exit\n",
    "    infoGain=1e-9\n",
    "    Left=[]\n",
    "    Right=[]\n",
    "    Threshold=None\n",
    "    Feature=None\n",
    "    \n",
    "        \n",
    "    for feature in range(RowSize-1):\n",
    "        allValsinFeature=[Data[x][feature] for x in range(len(Data))]\n",
    "        minVal=min(allValsinFeature)\n",
    "        maxVal=max(allValsinFeature)\n",
    "        Tvals = np.linspace(minVal,maxVal,6,endpoint=False)[1:]\n",
    "        for threshold in Tvals:\n",
    "            left=[]\n",
    "            right=[]\n",
    "            for inputs in Data:\n",
    "                if inputs[feature]<threshold:\n",
    "                    left.append([inputs[x] for x in [*range(0,feature),*range(feature+1,RowSize)]])\n",
    "                else:\n",
    "                    right.append([inputs[x] for x in [*range(0,feature),*range(feature+1,RowSize)]])\n",
    "            # print(\"threshold:\",threshold,\"left:\",left,\"right:\",right)\n",
    "            if(len(left)>0 and len(right)>0):\n",
    "                delEntropy=entropy(Data)-len(left)/len(Data)*entropy(left)-len(right)/len(Data)*entropy(right)\n",
    "                if delEntropy > infoGain:\n",
    "                    infoGain=delEntropy\n",
    "                    Left=left\n",
    "                    Right=right\n",
    "                    Threshold=threshold\n",
    "                    Feature=feature\n",
    "    return {\"threshold\":Threshold,\"feature\":Feature,\"Left\":Left,\"Right\":Right}\n",
    "\n",
    "\n",
    "class DecisionTree():\n",
    "    tree={}\n",
    "\n",
    "    def learn(self,Data):\n",
    "        tree={'feature':None, 'Value':None ,'threshold':None,'leftTree':{},'rightTree':{}}\n",
    "        if len(Data)>0:\n",
    "            # there should be at least 1 feature to split on and all the data shouldn't go to a single node\n",
    "            if len(Data[0])>=2 and entropy(Data)!=0:\n",
    "                best_split=BestSplit(Data)\n",
    "                tree['feature']=best_split[\"feature\"]\n",
    "                tree['threshold']=best_split[\"threshold\"]\n",
    "                # print(best_split)\n",
    "                leftTree={}\n",
    "                rightTree={}\n",
    "                leftTree=DecisionTree.learn(self,best_split[\"Left\"])\n",
    "                rightTree=DecisionTree.learn(self,best_split[\"Right\"])\n",
    "                tree['leftTree']=leftTree\n",
    "                tree['rightTree']=rightTree\n",
    "                # if not tree['feature'] and not tree['Value']:\n",
    "                    # tree['Value']=0\n",
    "                return tree \n",
    "            else :\n",
    "                tree['leftTree']= None\n",
    "                tree['rightTree']= None \n",
    "                rowsize=len(Data[0])\n",
    "                labels=[Data[x][rowsize-1] for x in range(len(Data))]\n",
    "                tree['Value']=max(labels,key=labels.count)\n",
    "                return tree    \n",
    "\n",
    "    def classify(self,tree,test_instance):\n",
    "        if tree:\n",
    "            if(tree['Value']) is not None:\n",
    "                return tree['Value'] \n",
    "            else:\n",
    "                f = test_instance[tree['feature']]\n",
    "                t =[test_instance[x] for x in [*range(0,tree['feature']),*range(tree['feature']+1,len(test_instance))]] \n",
    "                if(f<tree['threshold']):\n",
    "                    return self.classify(tree['leftTree'],t)\n",
    "                else:\n",
    "                    return self.classify(tree['rightTree'],t)if len(Data)>0:\n",
    "    \n",
    "\n",
    "def run_decision_tree():\n",
    "    BestTree={}\n",
    "    BestAccuracy=0\n",
    "    avgAccuracy=0\n",
    "\n",
    "    # Split training/test sets\n",
    "    K = 10\n",
    "    for kfold in range(K):\n",
    "        training_set = [x for i, x in enumerate(Tdata) if i % K != kfold]\n",
    "        test_set = [x for i, x in enumerate(Tdata) if i % K == kfold]\n",
    "        # print(len(training_set[0]))\n",
    "        Tree = DecisionTree()\n",
    "        # Construct a tree using training set\n",
    "        tree=Tree.learn( training_set )\n",
    "        # print(tree)\n",
    "        \n",
    "        # Classify the test set using the tree we just constructed\n",
    "        results = []\n",
    "        for instance in test_set:\n",
    "            result = Tree.classify(tree,instance[:-1] )\n",
    "            results.append( result == instance[-1])\n",
    "\n",
    "        # Accuracy\n",
    "        accuracy = float(results.count(True))/float(len(results))\n",
    "        if BestAccuracy<accuracy:\n",
    "            BestAccuracy=accuracy\n",
    "            BestTree=tree\n",
    "        avgAccuracy=avgAccuracy+accuracy\n",
    "        print(\"kfold: %d, accuracy: %.4f\" % (kfold+1, accuracy))\n",
    "        \n",
    "\n",
    "        # Writing results to a file (DO NOT CHANGE)\n",
    "        f = open(\"Chirag-Mehta-\"+\"result.txt\", \"w\")\n",
    "        f.write(\"accuracy: %.4f\" % accuracy)\n",
    "        f.close()\n",
    "    avgAccuracy=avgAccuracy/K\n",
    "    print(\"Best accuracy: %f, average accuracy: %f\" %(BestAccuracy,avgAccuracy))\n",
    "    f = open(\"Chirag-Mehta-\"+\"result.txt\", \"w\")\n",
    "    f.write(\"average accuracy: %.4f\" % avgAccuracy)\n",
    "    f.close()\n",
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "kfold: 1, accuracy: 0.8122\n",
      "kfold: 2, accuracy: 0.8388\n",
      "kfold: 3, accuracy: 0.8020\n",
      "kfold: 4, accuracy: 0.8000\n",
      "kfold: 5, accuracy: 0.8163\n",
      "kfold: 6, accuracy: 0.8000\n",
      "kfold: 7, accuracy: 0.8347\n",
      "kfold: 8, accuracy: 0.7918\n",
      "kfold: 9, accuracy: 0.7935\n",
      "kfold: 10, accuracy: 0.8037\n",
      "Best accuracy: 0.838776, average accuracy: 0.809306\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# improvement Strategy used: Gini Index\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# Storing the test and training data in TData\n",
    "with open(\"wine-dataset.csv\") as f:\n",
    "    next(f, None)\n",
    "    Tdata=[]\n",
    "    for line in csv.reader(f,delimiter=\",\"):\n",
    "        row=[float(x) for x in line]\n",
    "        Tdata.append(row)\n",
    "\n",
    "# gini Index function\n",
    "def gini(data):\n",
    "    \n",
    "    lastIndex=len(data[0])-1\n",
    "    n0,n1=0,0\n",
    "    for i in data:\n",
    "        if i[lastIndex]==0:\n",
    "            n0=n0+1\n",
    "        else:\n",
    "            n1=n1+1\n",
    "    p1=n0/(n1+n0)\n",
    "    p2=1-p1\n",
    "    # 1- sum p_i^2\n",
    "    return 1-p1**2-p2**2\n",
    "\n",
    "\n",
    "# The Best split function\n",
    "\n",
    "def BestSplit(Data):\n",
    "    RowSize=len(Data[0])\n",
    "    if RowSize<=1:\n",
    "        exit\n",
    "    # initializing the variables\n",
    "    infoGain=1e-9\n",
    "    Left=[]\n",
    "    Right=[]\n",
    "    Threshold=None\n",
    "    Feature=None\n",
    "    \n",
    "        \n",
    "    for feature in range(RowSize-1):\n",
    "        allValsinFeature=[Data[x][feature] for x in range(len(Data))]\n",
    "        minVal=min(allValsinFeature)\n",
    "        maxVal=max(allValsinFeature)\n",
    "        Tvals = np.linspace(minVal,maxVal,16,endpoint=False)[1:]\n",
    "        for threshold in Tvals:\n",
    "            left=[]\n",
    "            right=[]\n",
    "            for inputs in Data:\n",
    "                if inputs[feature]<threshold:\n",
    "                    left.append([inputs[x] for x in [*range(0,feature),*range(feature+1,RowSize)]])\n",
    "                else:\n",
    "                    right.append([inputs[x] for x in [*range(0,feature),*range(feature+1,RowSize)]])\n",
    "            if(len(left)>0 and len(right)>0):\n",
    "                delGini=gini(Data)-len(left)/len(Data)*gini(left)-len(right)/len(Data)*gini(right)\n",
    "                if delGini > infoGain:\n",
    "                    infoGain=delGini\n",
    "                    Left=left\n",
    "                    Right=right\n",
    "                    Threshold=threshold\n",
    "                    Feature=feature\n",
    "    return {\"threshold\":Threshold,\"feature\":Feature,\"Left\":Left,\"Right\":Right}\n",
    "\n",
    "\n",
    "class DecisionTree():\n",
    "    tree={}\n",
    "    #learn function\n",
    "    def learn(self,Data):\n",
    "        tree={'feature':None, 'Value':None ,'threshold':None,'leftTree':{},'rightTree':{}}\n",
    "        # if len(Data)==0 then the node is essentially not splitting\n",
    "        if len(Data)>0:\n",
    "            # there should be at least 1 feature to split on and all the data shouldn't go to a single node\n",
    "            if len(Data[0])>=2 and gini(Data)!=0:\n",
    "                best_split=BestSplit(Data)\n",
    "                tree['feature']=best_split[\"feature\"]\n",
    "                tree['threshold']=best_split[\"threshold\"]\n",
    "                leftTree={}\n",
    "                rightTree={}\n",
    "                leftTree=DecisionTree.learn(self,best_split[\"Left\"])\n",
    "                rightTree=DecisionTree.learn(self,best_split[\"Right\"])\n",
    "                tree['leftTree']=leftTree\n",
    "                tree['rightTree']=rightTree\n",
    "                return tree \n",
    "            else :\n",
    "                tree['leftTree']= None\n",
    "                tree['rightTree']= None \n",
    "                rowsize=len(Data[0])\n",
    "                labels=[Data[x][rowsize-1] for x in range(len(Data))]\n",
    "                tree['Value']=max(labels,key=labels.count)\n",
    "                return tree    \n",
    "\n",
    "    def classify(self,tree,test_instance):\n",
    "        if tree:\n",
    "            if(tree['Value']) is not None:\n",
    "                return tree['Value'] \n",
    "            else:\n",
    "                f = test_instance[tree['feature']]\n",
    "                t=[test_instance[x] for x in [*range(0,tree['feature']),*range(tree['feature']+1,len(test_instance))]] \n",
    "                if(f<tree['threshold']):\n",
    "                    return self.classify(tree['leftTree'],t)\n",
    "                else:\n",
    "                    return self.classify(tree['rightTree'],t)\n",
    "    \n",
    "\n",
    "def run_decision_tree():\n",
    "    BestTree={}\n",
    "    BestAccuracy=0\n",
    "    avgAccuracy=0\n",
    "    # Load data set\n",
    "    # random.shuffle(Tdata)\n",
    "\n",
    "    # Split training/test sets\n",
    "    K = 10\n",
    "    for kfold in range(K):\n",
    "        training_set = [x for i, x in enumerate(Tdata) if i % K != kfold]\n",
    "        test_set = [x for i, x in enumerate(Tdata) if i % K == kfold]\n",
    "        Tree = DecisionTree()\n",
    "        # Construct a tree using training set\n",
    "        tree=Tree.learn( training_set )\n",
    "        \n",
    "        # Classify the test set using the tree we just constructed\n",
    "        results = []\n",
    "        for instance in test_set:\n",
    "            result = Tree.classify(tree,instance[:-1] )\n",
    "            results.append( result == instance[-1])\n",
    "\n",
    "        # Accuracy\n",
    "        accuracy = float(results.count(True))/float(len(results))\n",
    "        if BestAccuracy<accuracy:\n",
    "            BestAccuracy=accuracy\n",
    "            BestTree=tree\n",
    "        avgAccuracy=avgAccuracy+accuracy\n",
    "        print(\"kfold: %d, accuracy: %.4f\" % (kfold+1, accuracy))\n",
    "        \n",
    "\n",
    "        # Writing results to a file (DO NOT CHANGE)\n",
    "        f = open(\"Chirag-Mehta-\"+\"result.txt\", \"w\")\n",
    "        f.write(\"accuracy: %.4f\" % accuracy)\n",
    "        f.close()\n",
    "    avgAccuracy=avgAccuracy/K\n",
    "    print(\"Best accuracy: %f, average accuracy: %f\" %(BestAccuracy,avgAccuracy))\n",
    "    f = open(\"Chirag-Mehta-\"+\"result.txt\", \"w\")\n",
    "    f.write(\"average accuracy: %.4f\" % avgAccuracy)\n",
    "    f.close()\n",
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "kfold: 1, accuracy: 0.8510\n",
      "kfold: 2, accuracy: 0.8347\n",
      "kfold: 3, accuracy: 0.8082\n",
      "kfold: 4, accuracy: 0.8224\n",
      "kfold: 5, accuracy: 0.8347\n",
      "kfold: 6, accuracy: 0.7959\n",
      "kfold: 7, accuracy: 0.8184\n",
      "kfold: 8, accuracy: 0.7898\n",
      "kfold: 9, accuracy: 0.7996\n",
      "kfold: 10, accuracy: 0.7873\n",
      "Best accuracy: 0.851020, average accuracy: 0.814201\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# Improvement strategy used: pre-prunning\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "with open(\"wine-dataset.csv\") as f:\n",
    "    next(f, None)\n",
    "    Tdata=[]\n",
    "    for line in csv.reader(f,delimiter=\",\"):\n",
    "        row=[float(x) for x in line]\n",
    "        Tdata.append(row)\n",
    "\n",
    "# entropy function\n",
    "def entropy(data):\n",
    "    \n",
    "    lastIndex=len(data[0])-1\n",
    "    n0,n1=0,0\n",
    "    for i in data:\n",
    "        if i[lastIndex]==0:\n",
    "            n0=n0+1\n",
    "        else:\n",
    "            n1=n1+1\n",
    "    p1=n0/(n1+n0)\n",
    "    p2=1-p1\n",
    "    # prunning\n",
    "    if max(p1,p2) > 0.95:\n",
    "        return 0\n",
    "    if p1 != 0:\n",
    "        p1=-p1*np.log(p1)\n",
    "    if p2 != 0:\n",
    "        p2=-p2*np.log(p2)\n",
    "    return p1+p2\n",
    "\n",
    "\n",
    "# The Best split function\n",
    "def BestSplit(Data):\n",
    "    RowSize=len(Data[0])\n",
    "    if RowSize<=1:\n",
    "        exit\n",
    "    infoGain=1e-9\n",
    "    Left=[]\n",
    "    Right=[]\n",
    "    Threshold=None\n",
    "    Feature=None\n",
    "    \n",
    "        \n",
    "    for feature in range(RowSize-1):\n",
    "        allValsinFeature=[Data[x][feature] for x in range(len(Data))]\n",
    "        minVal=min(allValsinFeature)\n",
    "        maxVal=max(allValsinFeature)\n",
    "        Tvals = np.linspace(minVal,maxVal,16,endpoint=False)[1:]\n",
    "        for threshold in Tvals:\n",
    "            left=[]\n",
    "            right=[]\n",
    "            for inputs in Data:\n",
    "                if inputs[feature]<threshold:\n",
    "                    left.append([inputs[x] for x in [*range(0,feature),*range(feature+1,RowSize)]])\n",
    "                else:\n",
    "                    right.append([inputs[x] for x in [*range(0,feature),*range(feature+1,RowSize)]])\n",
    "            if(len(left)>0 and len(right)>0):\n",
    "                delEntropy=entropy(Data)-len(left)/len(Data)*entropy(left)-len(right)/len(Data)*entropy(right)\n",
    "                if delEntropy > infoGain:\n",
    "                    infoGain=delEntropy\n",
    "                    Left=left\n",
    "                    Right=right\n",
    "                    Threshold=threshold\n",
    "                    Feature=feature\n",
    "    return {\"threshold\":Threshold,\"feature\":Feature,\"Left\":Left,\"Right\":Right}\n",
    "\n",
    "\n",
    "class DecisionTree():\n",
    "    tree={}\n",
    "\n",
    "    def learn(self,Data):\n",
    "        tree={'feature':None, 'Value':None ,'threshold':None,'leftTree':{},'rightTree':{}}\n",
    "        # if len(Data)==0 then the node is essentially not splitting\n",
    "        if len(Data)>0:\n",
    "            # there should be at least 1 feature to split on and all the data shouldn't go to a single node\n",
    "            if len(Data[0])>=2 and entropy(Data)!=0:\n",
    "                best_split=BestSplit(Data)\n",
    "                tree['feature']=best_split[\"feature\"]\n",
    "                tree['threshold']=best_split[\"threshold\"]\n",
    "                leftTree={}\n",
    "                rightTree={}\n",
    "                leftTree=DecisionTree.learn(self,best_split[\"Left\"])\n",
    "                rightTree=DecisionTree.learn(self,best_split[\"Right\"])\n",
    "                tree['leftTree']=leftTree\n",
    "                tree['rightTree']=rightTree\n",
    "                return tree \n",
    "            else :\n",
    "                tree['leftTree']= None\n",
    "                tree['rightTree']= None \n",
    "                rowsize=len(Data[0])\n",
    "                labels=[Data[x][rowsize-1] for x in range(len(Data))]\n",
    "                tree['Value']=max(labels,key=labels.count)\n",
    "                return tree    \n",
    "\n",
    "    def classify(self,tree,test_instance):\n",
    "        if tree:\n",
    "            if(tree['Value']) is not None:\n",
    "                return tree['Value'] \n",
    "            else:\n",
    "                f = test_instance[tree['feature']]\n",
    "                t=[test_instance[x] for x in [*range(0,tree['feature']),*range(tree['feature']+1,len(test_instance))]] \n",
    "                if(f<tree['threshold']):\n",
    "                    return self.classify(tree['leftTree'],t)\n",
    "                else:\n",
    "                    return self.classify(tree['rightTree'],t)\n",
    "    \n",
    "\n",
    "def run_decision_tree():\n",
    "    BestTree={}\n",
    "    BestAccuracy=0\n",
    "    avgAccuracy=0\n",
    "\n",
    "    # random.shuffle(Tdata)\n",
    "\n",
    "    # Split training/test sets\n",
    "    K = 10\n",
    "    for kfold in range(K):\n",
    "        training_set = [x for i, x in enumerate(Tdata) if i % K != kfold]\n",
    "        test_set = [x for i, x in enumerate(Tdata) if i % K == kfold]\n",
    "        Tree = DecisionTree()\n",
    "        tree=Tree.learn( training_set )\n",
    "        \n",
    "        # Classify the test set using the tree we just constructed\n",
    "        results = []\n",
    "        for instance in test_set:\n",
    "            result = Tree.classify(tree,instance[:-1] )\n",
    "            results.append( result == instance[-1])\n",
    "\n",
    "        # Accuracy\n",
    "        accuracy = float(results.count(True))/float(len(results))\n",
    "        if BestAccuracy<accuracy:\n",
    "            BestAccuracy=accuracy\n",
    "            BestTree=tree\n",
    "        avgAccuracy=avgAccuracy+accuracy\n",
    "        print(\"kfold: %d, accuracy: %.4f\" % (kfold+1, accuracy))\n",
    "        \n",
    "\n",
    "        # Writing results to a file (DO NOT CHANGE)\n",
    "        f = open(\"Chirag-Mehta-\"+\"result.txt\", \"w\")\n",
    "        f.write(\"accuracy: %.4f\" % accuracy)\n",
    "        f.close()\n",
    "    avgAccuracy=avgAccuracy/K\n",
    "    print(\"Best accuracy: %f, average accuracy: %f\" %(BestAccuracy,avgAccuracy))\n",
    "    f = open(\"Chirag-Mehta-\"+\"result.txt\", \"w\")\n",
    "    f.write(\"average accuracy: %.4f\" % avgAccuracy)\n",
    "    f.close()\n",
    "if __name__ == \"__main__\":\n",
    "    run_decision_tree()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "kfold: 1, accuracy: 0.8265\n",
      "kfold: 2, accuracy: 0.8327\n",
      "kfold: 3, accuracy: 0.8000\n",
      "kfold: 4, accuracy: 0.8184\n",
      "kfold: 5, accuracy: 0.8245\n",
      "kfold: 6, accuracy: 0.8122\n",
      "kfold: 7, accuracy: 0.8204\n",
      "kfold: 8, accuracy: 0.7898\n",
      "kfold: 9, accuracy: 0.8078\n",
      "kfold: 10, accuracy: 0.8323\n",
      "Best accuracy: 0.832653, average accuracy: 0.816457\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}